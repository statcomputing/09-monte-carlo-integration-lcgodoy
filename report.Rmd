---
title: "STAT 5361 - Homework #9"
subtitle: "Monte Carlo Integration"
author: "Lucas Godoy"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    number_sections: false
    includes:
      in_header: "utils/preamble.tex"
---

```{R setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      eval = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      fig.align = "center", 
                      fig.pos = "tb",
                      fig.width = 4,
                      fig.height = 3,
                      dpi = 300,
                      out.width = "90%")

opts <- options(knitr.kable.NA = "", 
                knitr.table.format = "latex",
                kableExtra.latex.load_packages = FALSE,
                digits = 4L)
```

```{R pkg-seed}
suppressWarnings(library(ggplot2)) ## avoiding ggplot2 warnings
library(kableExtra)

set.seed(2020)
```
```{R auxiliary-funs}
set.seed(2020)

dens_f <- function(x) (1/(5 * sqrt(2 * pi))) * x^2 * exp(-.5 * (x - 2)^2)

## is_f(10000, function(x) x^2, dens_f, dnorm, rnorm)

is_summary <- function(n, nrep = 1000, h, df, dg, rg, ...) {
    sim <- replicate(nrep, is_f(n, h, df, dg, rg, ...))

    c("mean" = mean(sim), "var" = var(sim))
}
```

## Problem 7.5.1

Find below the implementation for the Importance Sampling algorithm. Note that,
this function is heavily based to the one given in the lecure notes, it contains
only a slight difference when compared to the provided by the Professor which is
regarding the argument `...`, which is useful to include additional arguments
related to the chosen density $g(x)$.

```{R is-1, echo = TRUE}
is_f <- function(n, h, df, dg, rg, ...) {
    x <- rg(n, ...)
    mean( h(x) * df(x) / dg(x, ...) )
}
```
```{R is-est-vars-sampler1, cache = TRUE}
ssizes <- c(1000, 10000, 50000)
n_rep <- 1000

res_is1 <-
    res_is2 <-
        matrix(ncol = 3, nrow = 2,
               dimnames = list(c("mean", "var"),
                               sprintf("n = %d", ssizes)))

mean_var <- function(x) c(mean(x), var(x))

for(i in seq_along(ssizes)) {
    res_is1[, i] <- mean_var(replicate(1000,
                                       is_f(n = ssizes[i], h = function(x) x^2,
                                           df = dens_f, dg = dnorm,
                                           rg = rnorm)))
    
    res_is2[, i] <- mean_var(replicate(1000,
                                       is_f(n = ssizes[i], h = function(x) x^2,
                                           df = dens_f, dg = dnorm,
                                           rg = rnorm, mean = 2)))
}
```

The table below shows the estimates based on the importance sampling algorithm
using a standard normal as the $g$ density.
```{R tbl-is-1}
kbl(res_is1, booktabs = TRUE) %>%
    kable_styling(position = "center")
```

To improve the importance sampler, I used a $N(2, 1)$ as the $g(x)$
density. This was motivated by the fact that $g(x) \propto f(x)$ is a desirable
property and the exponent in the $f(x)$ density is the kernel of a normal
distribution with parameters $\mu = 2$ and $\sigma^2 = 1$.  

The table below contains the importance sampler estimates and variance of the
estimates using the proposed $g(x)$ density.
```{R tbl-is-2}
kbl(res_is2, booktabs = TRUE) %>%
    kable_styling(position = "center")
```

Clearly, the proposed $g(x)$ is a better choice to estimate $E[X^2]$ from the
$f(x)$ density, since the variace of the estimators are significantly smaller
than those based on the standard normal. Moreover, the estimates are closer to
each other for the different sample sizes. Note that, we can achieve best
estimates with a sample size 1000 using the proposed sample than the estimates
obtained by the given one with a sample size of 50000. 

## Problem 7.5.2

```{R auxiliary-funs-2}
## generate sample paths from a brownian motion
rBM <- function(n, tgrid, sigma) {
    tt <- c(0, tgrid)
    dt <- diff(tt)
    nt <- length(tgrid)
    dw <- matrix(rnorm(n * nt, sd = sigma * sqrt(dt)), n, nt, byrow = TRUE)
    t(apply(dw, 1, cumsum))
}

## geometric mean
geom_mean <- function(x) exp(mean(log(x)))

## true P_G (using the log-normal distribution)
callValLognorm <- function(S0, K, mu, sigma) {
    d <- (log(S0 / K) + mu + sigma^2) / sigma
    S0 * exp(mu + 0.5 * sigma^2) * pnorm(d) - K * pnorm(d - sigma)
}
```

The function below samples the path of the given $S(t)$.
```{R sample-path-1, echo = TRUE}
r_s <- function(n, tgrid, sigma, r, S0) {
    wt <- rBM(n, tgrid, sigma)
    ## payoff of call option on arithmetic average
    nt <- length(tgrid)
    St <- S0 * exp((r - sigma^2 / 2) * matrix(tgrid, n, nt, byrow = TRUE) + wt)
    return(St)
}
```

On the other hand, calculates the correlations between the given
coefficients. Note that, I have implemented a function to calculate the
geometric mean to make to code cleaner.
```{R cor-coefs, echo = TRUE}
cor_coefs <- function(n, tgrid, sigma, r, S0, K) {
    st <- r_s(n = n, tgrid = tgrid, sigma = sigma, r = r, S0 = S0)
    tt <- tgrid[length(tgrid)]
    sa <- apply(st, 1, mean)
    sg <- apply(st, 1, geom_mean)
    sT <- apply(st, 1, function(x) x[length(x)])
    pa <- exp(- r * tt) * pmax((sa - K), 0)
    pe <- exp(- r * tt) *pmax((sT - K), 0)
    pg <- exp(- r * tt) *pmax((sg - K), 0)
    c("pa_pe" = cor(pa, pe),
      "pa_pg" = cor(pa, pg),
      "pe_pg" = cor(pe, pg))
}

```

The table below displays all the possible correlation between the
coefficients. There is a clear pattern on which as the $K$ parameter is
increased the correlation between the coefficients decreases.
```{R problem-b, cache = TRUE}
## sample path parameters
r <- 0.05; sigma <- 0.5; S0 <- 1; K <- 1.1; .n  <- 12
t_grid <- seq(from = 0, to = 1, length.out = .n + 1)[-1]

n_repl <- 5000
k <- seq(from = 1.1, to = 1.5, by = .1)

out_k <- purrr::map_df(k,
                       .f = ~ cor_coefs(n = n_repl,
                                        tgrid = t_grid, S0 = S0,
                                        r = r, sigma = sigma, K = .))

out_k <- cbind(data.frame(k = k),
               out_k)

colnames(out_k) <- c("$K$", "$Cor(P_A, P_E)$",
                     "$Cor(P_A, P_G)$", "$Cor(P_E, P_G)$")

kbl(out_k, booktabs = TRUE, escape = FALSE) %>%
    kable_styling(position = "center")
```

The table below displays the correlation of the coefficients for simulated
sample paths with different $\sigma$ parameters. The pattern here is the
opposite as the observed for the $K$'s, that is, as we increase $\sigma$ the
correlation between the parameters is increased. For some of them this increase
in the correlation magnitude is dramatic.
```{R problem-c, cache = TRUE}
sigs <- seq(from = .2, to = .5, by = .1)

out_s <- purrr::map_df(sigs,
                       .f = ~ cor_coefs(n = n_repl,
                                        tgrid = t_grid, S0 = 1,
                                        r = r, sigma = ., K = k[5]))

out_s <- cbind(data.frame(sigma = sigs),
               out_s)

colnames(out_s) <- c("$\\sigma$", "$Cor(P_A, P_E)$",
                     "$Cor(P_A, P_G)$", "$Cor(P_E, P_G)$")

kbl(out_s, booktabs = TRUE, escape = FALSE) %>%
    kable_styling(position = "center")
```

The table below shows a similar simulation scenario than the previous two
mentioned here. In here, what is varying is the $T$. In here, as we increase
$T$, two correlation coefficients are always increased, while the $Cor(P_A,
P_G)$ increases up to $T = 1.3$. From $T = 1.3$ to $T = 1.6$ it presented a
slight decreaase. 
```{R problem-d}
fT <- c(.4, .7, 1, 1.3, 1.6)

out_t <- purrr::map_df(fT,
                       .f = ~ cor_coefs(n = n_repl,
                                        tgrid = seq(from = 0, to = .,
                                                    length.out = .n + 1)[-1],
                                        S0 = 1, r = r, sigma = .5,
                                        K = k[5]))

out_t <- cbind(data.frame("T" = fT),
               out_t)

colnames(out_t) <- c("$T$", "$Cor(P_A, P_E)$",
                     "$Cor(P_A, P_G)$", "$Cor(P_E, P_G)$")

kbl(out_t, booktabs = TRUE, escape = FALSE) %>%
    kable_styling(position = "center")
```

For the last part of the problem, I have implemented the following function. If
the argument `return_sd` is set to `FALSE`, then the MC estimates of $E[P_A]$
are returned instead of their standard deviation.
```{R est-final}
est_sd_pa <- function(n, tgrid, sigma, r, S0, K, return_sd = TRUE) {
    st <- r_s(n = n, tgrid = tgrid, sigma = sigma, r = r, S0 = S0)
    nt <- length(tgrid)
    tt <- tgrid[nt]
    sa <- apply(st, 1, mean)
    sg <- apply(st, 1, geom_mean)
    pa <- exp(- r * tt) * pmax((sa - K), 0)
    pg <- exp(- r * tt) *pmax((sg - K), 0)
    tbar <- mean(tgrid)
    sBar2 <- sigma^2 / nt^2 / tbar * sum( (2 * seq(nt) - 1) * rev(tgrid) )
    pg_true <- callValLognorm(S0, K, (r - 0.5 * sigma^2) * tbar,
                              sqrt(sBar2 * tbar))
    b <- cov(pa, pg) / var(pg)

    if(return_sd) {
    pa_naive <- sd(pa)
    pa_cntrl <- sd(pa - b * (pg - pg_true))
    
    c("pa_naive" = pa_naive,
      "pa_cntrl" = pa_cntrl)
    } else {
         c("pa_naive" = mean(pa),
           "pa_cntrl" = mean(pa - b * (pg - pg_true)))
    }
}
```

The table below contains the standard deviations of the estimates of $E[P_A]$
both using the control variate and with the standard MC integration approach.
Clearly, as expected, the second one provides estimates with smaller standard
deviations.
```{R problem-e, cache = TRUE}
sim <- replicate(5000,
                 est_sd_pa(n = 12, tgrid = t_grid,
                           sigma = .4, r = r, S0 = 1, K = 1.5,
                           return_sd = TRUE))
```

```{R problem-e2}
sim <- apply(sim, 1, mean, na.rm = TRUE)
names(sim) <- c("$\\mbox{SD}_{MC}(E[P_A])$", "$\\mbox{SD}_{CV}(E[P_A])$")

kbl(as.data.frame(t(sim)), booktabs = TRUE, escape = FALSE) %>%
     kable_styling(position = "center")
```
